{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DIMENSIONALITY REDUCTION TECHNIQUES :\n\n1. Missing Value Ratio\n2. Low variance filter\n3. High Correlation filter\n4. Random forest / ensemble trees\n5. Backward Feature Elimination\n6. Forward Feature Selection \n7. PCA\n8. Backward Feature elimination + missing value ratio\n9. Forward Feature Selection + missing value ratio\n10. t-SNE\n11. LDA\n12. Neural autoencoder\n13. Independant component analysis\n14. UMAP\n15. Factor Analysis\n16. Methods Based on Projection\n17. TruncatedSVD\n18. GaussianRandomProjection\n19. SparseRandomPorjection","metadata":{}},{"cell_type":"markdown","source":"## Dimensionality reduction can be done on 2 ways :\n\n1. Feature selection -> Keeping only the most relevant features from dataset\n2. Dimensionality reduction -> By finding smaller set of variables, each combination of input variables, containing basically the same infos as input variables.\n","metadata":{}},{"cell_type":"markdown","source":"PCA","metadata":{}},{"cell_type":"markdown","source":"1. The goal of PCA is to extract important information from data and express this info in a set of summary indices called principal components.\n2. PCA finds lines, planes and hyperplanes in K-dimensional space that minimize residual variance by least squares and maximise variance of co-ordinates of line.\n3. 10D data gives 10 components, but the idea is to put max possible info in the first component, then on second and so on.\n\nHOW IT WORKS :\n1. Standardize the values, including mean centering so that each value provide equal contribution.\n\n2. The first principcal component is computed. The component is line in K-dimensional variable space that best approximates the data in the least square sense. \n\n3. Each observation is projected onto this line to get a coordinate value along a PC-line. This value is called score.\n\n4. Usually 1 principal component is insufficient so a second principal component is calculated. The second is orthogonal to the first and reflects the second largest source of variation in data.\n\n5. 2 PCs define a plane. The coordinate values of observations on this plane are called scores.\n\n6. In graph of 2 PCs, variable contributing similar info are grouped together. (one increase, the other also increase)\n\n7. When variables are negatively correlated, they are positioned on the opposite side of plot origin (diagnally opposed quadrants ).\n\n8. The further away from plot of origin the variable lies, the stronger the impact the variable has on model.\n\n9. In situations with a lot of predictors p and relatively few data points n (e.g. when pâ‰ˆn or even p>n), ordinary regression will overfit and needs to be regularized. Principal component regression (PCR) can be seen as one way to regularize the regression and will tend to give superior results.\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"t-SNE","metadata":{"trusted":true}},{"cell_type":"markdown","source":"1. If we have a graph of clusters and we just projected the data onto one of the axes, we would just get a big mess that does not preserve the original cluster.\n2. t-SNE find a way to project data into a low-D space, so that the clustering in high-D space is preserved.\n3. For t-SNE, points moves a little bit at a time until they have formed clusters.\n\nHOW IT WORKS :\n1. First, measure distance btwn 2 points on graph. Then plot the distance on normal curve that is centered on point of interest.\n2. Draw a line from point to curve. The length of the line is unscaled similarity.\n3. Find the unscaled similarity for all the points from point of interest.\n4. Using normal distribution means distant points hv low similarity values. Close points from center hv high similarity values.\n5. Scale the unscaled similarity so that they add up to 1. \n6. t-SNE has perplexity parameter, which equal to the expected density.\n7. The width of normal distribution is based on density of surrounding data points. \n8. Finally u end up with similarity matrix. \n9. Move point a little bit at a time until they have formed clusters.","metadata":{"trusted":true}},{"cell_type":"markdown","source":"- For both PCA and t-SNE, they are dimenstionality reduction algorithms. t-sne is not an clustering algorithm. Because it maps multi-D data to lower-D space and the inputs are no longer identifiable.\n\n- You cannot make an inference based only on the outputs of t-sne, its mainly a data exploration and visualization technique. But its output can be used as input for other classification algorithm to find clusters.\n\n- t-sne outputs better results than PCA and other dimentionality reduction models. This is because linear methods are not good at modelling curved manifolds.\n\n- linear models focuses on preserving distances btwn widely seperated data points rather than on preserving nearby data points.\n\n- So for a curved spiral manifold, t-sne would be better cuz it would preserve the info while pca would just fit a straight line.\n\n- It is highly recommended to use other dimensionality reduction method to reduce number of dimension to a lower amount if the number of features are too high before using t-SNE (ex: 50). This will supress some noise and speed up computation.\n\n- Sometimes in t-SNE different runs with the same hyperparameters may produce different results hence multiple plots must be observed before making any assessment with t-SNE, while this is not the case with PCA.\n\n- Since PCA is a linear algorithm, it will not be able to interpret the complex polynomial relationship between features while t-SNE is made to capture exactly that.\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"HYPERPARAMETERS TUNING FOR T-SNE\n\n1. Perplexity - Smooth measure of effective number of neighbours. Typical values are 5 to 50.\n2. n_iter - Number of iteration. Min is 250.\n3. learning_rate - The learning rate for t-sne is in range (10, 1000 )\n4. early_exaggeration - Controls how tight natural clusters in the original space are in the embedded space and\nhow much space will be between them.","metadata":{"trusted":true}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}